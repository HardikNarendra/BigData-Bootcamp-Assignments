{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Mock Test 1 - Big Data - PPT - iNeuron"
      ],
      "metadata": {
        "id": "8-oH9cq9cFTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Write a PySpark code to read a CSV file named \"employees.csv\" containing the following columns: \"employee_id\", \"name\", \"age\", \"department\". Display the top 10 records from the DataFrame."
      ],
      "metadata": {
        "id": "JFB5fSyVcV2k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadCSV\").getOrCreate()\n",
        "\n",
        "# Read the CSV file into a DataFrame\n",
        "df = spark.read.csv(\"employees.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Display the top 10 records\n",
        "df.show(10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tbs3wuGccVW",
        "outputId": "78092573-ec64-4224-e7ac-68c201fb134c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "+-----------+--------+----+----------------+\n",
            "|employee_id|   name |age |      department|\n",
            "+-----------+--------+----+----------------+\n",
            "|          1| Hardik |  15|             CSE|\n",
            "|          2|Narendra|  25|              IT|\n",
            "|          3|Ramanlal|  35|      Electrical|\n",
            "|          4|   Niwas|  65|           Civil|\n",
            "|          5|   ashok|  25|       Aerospace|\n",
            "|          6|   Rahuk|  85|      mechanical|\n",
            "|          7|   Meet |  65|      Electronic|\n",
            "|          8|   Darji|  34|Hotel management|\n",
            "|          9|  Bishwa|  25|             CSE|\n",
            "|         10|   Jeety|  26|              IT|\n",
            "+-----------+--------+----+----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.  Given a PySpark DataFrame named \"sales_data\" with columns \"product_name\" and \"revenue\", write a code to calculate the total revenue for each product and display the result in descending order."
      ],
      "metadata": {
        "id": "5xXOxDYue-85"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ProductRevenue\").getOrCreate()\n",
        "\n",
        "sales_data = spark.read.csv(\"sales_data.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Calculate total revenue for each product\n",
        "product_revenue = sales_data.groupBy(\"product_name\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "\n",
        "# Order by total revenue in descending order\n",
        "product_revenue = product_revenue.orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "# Display the result\n",
        "product_revenue.show()\n"
      ],
      "metadata": {
        "id": "qXbponblct6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Write a PySpark code to read a JSON file named \"students.json\" containing student records with the following schema: \"name\" (string), \"age\" (integer), \"grade\" (string). Filter the DataFrame to include only students whose age is greater than 18."
      ],
      "metadata": {
        "id": "yDDRtujnfFYP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "# Read the JSON file into a DataFrame\n",
        "df = spark.read.json(\"students.json\")\n",
        "\n",
        "# Filter the DataFrame to include only students whose age is greater than 18\n",
        "filtered_df = df.filter(df.age > 18)\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "filtered_df.show()\n"
      ],
      "metadata": {
        "id": "So0AGQ14e9uA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Consider a PySpark DataFrame named \"transactions\" with columns \"transaction_id\", \"user_id\", and \"amount\". Write a code to calculate the average transaction amount for each user and display the result."
      ],
      "metadata": {
        "id": "ffgYW5FZfKdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import avg\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"AverageTransaction\").getOrCreate()\n",
        "\n",
        "# Calculate average transaction amount for each user\n",
        "average_amount = transactions.groupBy(\"user_id\").agg(avg(\"amount\").alias(\"average_amount\"))\n",
        "\n",
        "# Display the result\n",
        "average_amount.show()\n"
      ],
      "metadata": {
        "id": "IAMP9kuKfNkb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Given a PySpark DataFrame named \"logs\" with columns \"timestamp\" (timestamp) and \"event\" (string), write a code to count the number of events that occurred in each hour and display the result sorted by the hour."
      ],
      "metadata": {
        "id": "tYVEOsvzfOAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import hour\n",
        "from pyspark.sql.functions import count\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"EventCount\").getOrCreate()\n",
        "\n",
        "# Extract the hour from the timestamp column\n",
        "logs = logs.withColumn(\"hour\", hour(logs.timestamp))\n",
        "\n",
        "# Count the number of events for each hour\n",
        "event_count = logs.groupBy(\"hour\").agg(count(\"event\").alias(\"event_count\"))\n",
        "\n",
        "# Sort the result by the hour\n",
        "sorted_event_count = event_count.orderBy(\"hour\")\n",
        "\n",
        "# Display the result\n",
        "sorted_event_count.show()\n"
      ],
      "metadata": {
        "id": "nFrJq82RfR0Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.  Retrieve all the customers from the \"Customers\" table whose age is greater than 25 and have made at least one purchase."
      ],
      "metadata": {
        "id": "LnaYqPERfSe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"CustomerAnalysis\").getOrCreate()\n",
        "\n",
        "# Read the \"Customers\" table into a DataFrame\n",
        "customers_df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "# Filter the DataFrame to include customers whose age is greater than 25 and have made at least one purchase\n",
        "filtered_customers_df = customers_df.filter(customers_df.age > 25).filter(customers_df.purchases > 0)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "filtered_customers_df.show()\n"
      ],
      "metadata": {
        "id": "qCZnN7VzfVO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Find the total number of orders placed by each customer and display the results in descending order of the number of orders."
      ],
      "metadata": {
        "id": "VKAVtK9cfVpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import desc\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"OrderAnalysis\").getOrCreate()\n",
        "\n",
        "# Read the \"Orders\" table into a DataFrame\n",
        "orders_df =spark.read.csv(\"orders.csv\", header=True, inferSchema=True)\n",
        "\n",
        "\n",
        "# Calculate the total number of orders placed by each customer\n",
        "customer_order_count = orders_df.groupBy(\"customer_id\").agg(count(\"order_id\").alias(\"order_count\"))\n",
        "\n",
        "# Sort the result by the number of orders in descending order\n",
        "sorted_order_count = customer_order_count.orderBy(desc(\"order_count\"))\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "sorted_order_count.show()\n"
      ],
      "metadata": {
        "id": "2Hdib3ebfY5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Retrieve the names of all products that are currently out of stock from the \"Products\" table."
      ],
      "metadata": {
        "id": "EFMl53jrfbvy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"ProductAnalysis\").getOrCreate()\n",
        "\n",
        "# Read the \"Products\" table into a DataFrame\n",
        "products_df = spark.read.csv(\"products.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Filter the DataFrame to include only products that are currently out of stock\n",
        "out_of_stock_products_df = products_df.filter(products_df.stock_quantity == 0)\n",
        "\n",
        "# Select and display the names of the out-of-stock products\n",
        "out_of_stock_product_names = out_of_stock_products_df.select(\"product_name\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "out_of_stock_product_names.show()\n"
      ],
      "metadata": {
        "id": "L9NM5yYjfc8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Calculate the average price of all products in each category and display the results along with the category name."
      ],
      "metadata": {
        "id": "uGUzzOQHfdrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the average price of products in each category\n",
        "avg_price_by_category = products_df.groupBy(\"category\").agg(avg(\"price\").alias(\"average_price\"))\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "avg_price_by_category.show()\n"
      ],
      "metadata": {
        "id": "fMUhvMEIfg84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Retrieve the top 5 customers who have spent the highest total amount on purchases."
      ],
      "metadata": {
        "id": "Vf1ma163fhhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import sum\n",
        "\n",
        "\n",
        "# Join the \"Customers\" and \"Orders\" DataFrames based on customer_id\n",
        "joined_df = customers_df.join(orders_df, customers_df.customer_id == orders_df.customer_id)\n",
        "\n",
        "# Calculate the total amount spent by each customer\n",
        "customer_spending = joined_df.groupBy(\"customer_id\", \"customer_name\").agg(sum(\"amount\").alias(\"total_spending\"))\n",
        "\n",
        "# Sort the result by total spending in descending order\n",
        "sorted_spending = customer_spending.orderBy(desc(\"total_spending\"))\n",
        "\n",
        "# Retrieve the top 5 customers with the highest total spending\n",
        "top_5_customers = sorted_spending.limit(5)\n",
        "\n",
        "# Display the resulting DataFrame\n",
        "top_5_customers.show()\n"
      ],
      "metadata": {
        "id": "xAQIQ4pqfkVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}